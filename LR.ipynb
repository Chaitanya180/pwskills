{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e243a5",
   "metadata": {},
   "source": [
    "**Q1. Simple Linear Regression vs. Multiple Linear Regression:**\n",
    "\n",
    "- **Simple Linear Regression:**\n",
    "  - In simple linear regression, there is a single independent variable predicting a dependent variable.\n",
    "  - Formula: \\(y = \\beta_0 + \\beta_1x + \\varepsilon\\), where \\(y\\) is the dependent variable, \\(x\\) is the independent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(\\varepsilon\\) is the error term.\n",
    "  - Example: Predicting a student's exam score (\\(y\\)) based on the number of hours they studied (\\(x\\)).\n",
    "\n",
    "- **Multiple Linear Regression:**\n",
    "  - In multiple linear regression, there are two or more independent variables predicting a dependent variable.\n",
    "  - Formula: \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\varepsilon\\), where \\(y\\) is the dependent variable, \\(x_1, x_2, \\ldots, x_n\\) are independent variables, \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\beta_2, \\ldots, \\beta_n\\) are slopes, and \\(\\varepsilon\\) is the error term.\n",
    "  - Example: Predicting a house price (\\(y\\)) based on features like square footage (\\(x_1\\)), number of bedrooms (\\(x_2\\)), and location (\\(x_3\\)).\n",
    "\n",
    "**Q2. Assumptions of Linear Regression:**\n",
    "1. **Linearity:** The relationship between independent and dependent variables is linear.\n",
    "2. **Independence:** Residuals (errors) are independent of each other.\n",
    "3. **Homoscedasticity:** Residuals have constant variance.\n",
    "4. **Normality of Residuals:** Residuals are normally distributed.\n",
    "5. **No Multicollinearity:** Independent variables are not highly correlated.\n",
    "\n",
    "To check these assumptions:\n",
    "- Plot residuals against predicted values.\n",
    "- Use a residual vs. fitted value plot for homoscedasticity.\n",
    "- Perform statistical tests for normality.\n",
    "- Check variance inflation factor (VIF) for multicollinearity.\n",
    "\n",
    "**Q3. Interpretation of Slope and Intercept:**\n",
    "- **Slope (\\(\\beta_1\\)):** It represents the change in the dependent variable for a one-unit change in the independent variable, holding other variables constant.\n",
    "- **Intercept (\\(\\beta_0\\)):** It represents the value of the dependent variable when all independent variables are zero.\n",
    "\n",
    "Example: In a salary prediction model, the slope for years of experience (\\(\\beta_1\\)) might be 2000, indicating a $2000 increase in salary for each additional year of experience. The intercept (\\(\\beta_0\\)) could be $30,000, representing the estimated salary for someone with zero years of experience.\n",
    "\n",
    "**Q4. Gradient Descent:**\n",
    "- **Concept:** Gradient descent is an optimization algorithm used to minimize the cost function in machine learning. It iteratively adjusts model parameters to reach the minimum of the cost function.\n",
    "- **Usage:** In machine learning, it is employed to find the optimal weights in a model by adjusting them in the direction that minimizes the cost.\n",
    "\n",
    "**Q5. Multiple Linear Regression Model:**\n",
    "- **Description:** It extends simple linear regression to include multiple independent variables.\n",
    "- **Formula:** \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\varepsilon\\).\n",
    "- **Difference:** In multiple regression, there are multiple predictors influencing the dependent variable.\n",
    "\n",
    "**Q6. Multicollinearity:**\n",
    "- **Concept:** Multicollinearity occurs when independent variables in a multiple regression model are highly correlated.\n",
    "- **Detection:** Check VIF (variance inflation factor) for each variable; high VIF indicates multicollinearity.\n",
    "- **Addressing:** Remove one of the correlated variables or use dimensionality reduction techniques.\n",
    "\n",
    "**Q7. Polynomial Regression Model:**\n",
    "- **Description:** Polynomial regression is a type of regression analysis where the relationship between the independent and dependent variables is modeled as an nth-degree polynomial.\n",
    "- **Difference:** Unlike linear regression, it can capture non-linear relationships.\n",
    "\n",
    "**Q8. Advantages and Disadvantages of Polynomial Regression:**\n",
    "- **Advantages:**\n",
    "  - Captures non-linear patterns.\n",
    "  - More flexible than linear regression.\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - Prone to overfitting.\n",
    "  - Interpretability can be challenging.\n",
    "  - More sensitive to outliers.\n",
    "  \n",
    "**Preference for Polynomial Regression:**\n",
    "- Use polynomial regression when the relationship between variables is non-linear and cannot be adequately captured by a linear model. However, be cautious about overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302e8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
