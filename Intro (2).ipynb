{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "90385297-aa6f-4ce1-89b4-a11ffa655a3f",
      "cell_type": "markdown",
      "source": "1. **KNN Algorithm**: K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks. It classifies or predicts the label of a data point based on the majority label of its k nearest neighbors in the feature space.\n\n2. **Choosing the Value of K**: The value of K in KNN is typically chosen through techniques such as cross-validation, where different values of K are tested and the one that gives the best performance on a validation set is selected.\n\n3. **Difference between KNN Classifier and Regressor**:\n   - **KNN Classifier**: Predicts the class label of a data point by a majority vote of its nearest neighbors.\n   - **KNN Regressor**: Predicts the numerical value of a data point by averaging the values of its nearest neighbors.\n\n4. **Measuring Performance of KNN**: Performance of KNN can be measured using metrics like accuracy, precision, recall, F1-score for classification tasks, and metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), or R-squared for regression tasks.\n\n5. **Curse of Dimensionality**: In KNN, as the number of dimensions (features) increases, the distance between data points becomes less meaningful, leading to increased computational complexity and decreased performance, known as the curse of dimensionality.\n\n6. **Handling Missing Values**: Missing values in KNN can be handled by imputing them with the mean, median, or mode of the respective feature across the dataset, or using more advanced imputation techniques such as k-Nearest Neighbors imputation.\n\n7. **Comparison of KNN Classifier and Regressor**:\n   - **Classifier**: Suitable for classification problems where the output is discrete classes.\n   - **Regressor**: Suitable for regression problems where the output is a continuous numerical value.\n\n8. **Strengths and Weaknesses**:\n   - **Strengths**: Simple to implement, non-parametric, robust to noisy data.\n   - **Weaknesses**: Computationally expensive for large datasets, sensitive to irrelevant features and the choice of distance metric. Techniques like dimensionality reduction, feature selection, and distance metric optimization can address some of these weaknesses.\n\n9. **Difference between Euclidean and Manhattan Distance**:\n   - **Euclidean Distance**: Measures the straight-line distance between two points in a Euclidean space.\n   - **Manhattan Distance**: Measures the distance between two points as the sum of the absolute differences of their coordinates.\n\n10. **Role of Feature Scaling**: Feature scaling ensures that all features contribute equally to the distance computation in KNN. Since KNN relies on distance metrics, features with larger scales may dominate the distance calculation, leading to biased results. Common scaling techniques include Min-Max scaling or Standardization (Z-score normalization).",
      "metadata": {}
    },
    {
      "id": "9cef0b91-4b93-4669-8fcf-8604db7ca523",
      "cell_type": "markdown",
      "source": "Q1. The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN lies in how they measure distance between data points. Euclidean distance calculates the shortest straight line between two points in a multidimensional space, while Manhattan distance calculates the distance as the sum of the absolute differences between the coordinates of the points.\n\nThis difference can affect KNN's performance as it determines how \"closeness\" between data points is defined. For example, Euclidean distance is sensitive to the scale of the features, meaning features with larger scales may dominate the distance calculation. On the other hand, Manhattan distance is less sensitive to scale since it measures distance by summing the absolute differences. Depending on the dataset and its characteristics, one metric may perform better than the other.\n\nQ2. Choosing the optimal value of k for a KNN classifier or regressor involves a trade-off between bias and variance. A smaller value of k leads to more complex models with low bias but high variance, while a larger value of k results in simpler models with high bias but low variance. \n\nTechniques for determining the optimal k value include cross-validation, grid search, and using performance metrics such as accuracy (for classification) or mean squared error (for regression) on a validation set.\n\nQ3. The choice of distance metric can significantly affect the performance of a KNN classifier or regressor. Euclidean distance is commonly used when the data is continuous and the relationships between features are linear, while Manhattan distance may be preferred when dealing with categorical data or when the features have different scales. \n\nIn situations where the features are of different types or have varying importance, a custom distance metric or a combination of multiple metrics may be used to improve performance.\n\nQ4. Common hyperparameters in KNN classifiers and regressors include k (number of neighbors), weights (uniform or distance-based weighting), and the choice of distance metric (e.g., Euclidean, Manhattan). These hyperparameters affect the model's complexity, bias-variance trade-off, and sensitivity to feature scales.\n\nHyperparameter tuning techniques such as grid search, random search, or Bayesian optimization can be employed to find the optimal combination of hyperparameters. Cross-validation is typically used to evaluate model performance for different hyperparameter values.\n\nQ5. The size of the training set can significantly affect the performance of a KNN classifier or regressor. With a smaller training set, the model may suffer from high variance and overfitting, especially when k is small. Conversely, a larger training set can lead to better generalization but may increase computational complexity.\n\nTechniques for optimizing the size of the training set include cross-validation to assess model performance with different training set sizes, and techniques such as bootstrapping or resampling to generate additional training data when the original dataset is small.\n\nQ6. Some potential drawbacks of using KNN as a classifier or regressor include:\n\n- High computational complexity, especially with large datasets.\n- Sensitivity to irrelevant features or noisy data.\n- Difficulty in handling high-dimensional data efficiently.\n- Imbalanced data can lead to biased predictions.\n\nTo overcome these drawbacks, one might consider dimensionality reduction techniques like PCA or feature selection to reduce the number of features. Preprocessing techniques such as normalization or scaling can help mitigate the impact of feature scales. Additionally, using ensemble methods or weighted KNN can improve robustness and performance. Regularization techniques can also be employed to handle noisy data and prevent overfitting.",
      "metadata": {}
    },
    {
      "id": "d908f9ba-3f65-4c99-a6d0-71b754f8ca9c",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}