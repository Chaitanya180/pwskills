{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "90385297-aa6f-4ce1-89b4-a11ffa655a3f",
      "cell_type": "markdown",
      "source": "1. **KNN Algorithm**: K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks. It classifies or predicts the label of a data point based on the majority label of its k nearest neighbors in the feature space.\n\n2. **Choosing the Value of K**: The value of K in KNN is typically chosen through techniques such as cross-validation, where different values of K are tested and the one that gives the best performance on a validation set is selected.\n\n3. **Difference between KNN Classifier and Regressor**:\n   - **KNN Classifier**: Predicts the class label of a data point by a majority vote of its nearest neighbors.\n   - **KNN Regressor**: Predicts the numerical value of a data point by averaging the values of its nearest neighbors.\n\n4. **Measuring Performance of KNN**: Performance of KNN can be measured using metrics like accuracy, precision, recall, F1-score for classification tasks, and metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), or R-squared for regression tasks.\n\n5. **Curse of Dimensionality**: In KNN, as the number of dimensions (features) increases, the distance between data points becomes less meaningful, leading to increased computational complexity and decreased performance, known as the curse of dimensionality.\n\n6. **Handling Missing Values**: Missing values in KNN can be handled by imputing them with the mean, median, or mode of the respective feature across the dataset, or using more advanced imputation techniques such as k-Nearest Neighbors imputation.\n\n7. **Comparison of KNN Classifier and Regressor**:\n   - **Classifier**: Suitable for classification problems where the output is discrete classes.\n   - **Regressor**: Suitable for regression problems where the output is a continuous numerical value.\n\n8. **Strengths and Weaknesses**:\n   - **Strengths**: Simple to implement, non-parametric, robust to noisy data.\n   - **Weaknesses**: Computationally expensive for large datasets, sensitive to irrelevant features and the choice of distance metric. Techniques like dimensionality reduction, feature selection, and distance metric optimization can address some of these weaknesses.\n\n9. **Difference between Euclidean and Manhattan Distance**:\n   - **Euclidean Distance**: Measures the straight-line distance between two points in a Euclidean space.\n   - **Manhattan Distance**: Measures the distance between two points as the sum of the absolute differences of their coordinates.\n\n10. **Role of Feature Scaling**: Feature scaling ensures that all features contribute equally to the distance computation in KNN. Since KNN relies on distance metrics, features with larger scales may dominate the distance calculation, leading to biased results. Common scaling techniques include Min-Max scaling or Standardization (Z-score normalization).",
      "metadata": {}
    }
  ]
}